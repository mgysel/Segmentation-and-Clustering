# Determine percentage of total
sum_sales_percentage <- data.frame(store=sum_sales$Store,
year=sum_sales$Year,
dry_grocery=100*sum_sales$dry_grocery/sum_sales$total,
dairy=100*sum_sales$dairy/sum_sales$total,
frozen_food=100*sum_sales$frozen_food/sum_sales$total,
meat=100*sum_sales$meat/sum_sales$total,
produce=100*sum_sales$produce/sum_sales$total,
floral=100*sum_sales$floral/sum_sales$total,
deli=100*sum_sales$deli/sum_sales$total,
bakery=100*sum_sales$bakery/sum_sales$total,
general_merchandise=100*sum_sales$general_merchandise/sum_sales$total)
# Check to make sure each column adds up to 100
sum_sales_percentage$total <- sum_sales_percentage$dry_grocery + sum_sales_percentage$dairy + sum_sales_percentage$frozen_food + sum_sales_percentage$meat + sum_sales_percentage$produce + sum_sales_percentage$floral + sum_sales_percentage$deli + sum_sales_percentage$bakery + sum_sales_percentage$general_merchandise
# Obtain 2015 sales data
sum_sales_percentage_2015 <- subset(sum_sales_percentage, year==2015)
# Load dependencies
library(NbClust)
# Determine optimal number of clusters
NbClust(sum_sales_percentage_2015[3:11], method='kmeans', index='ch')
summary <- NbClust(sum_sales_percentage_2015[3:11], method='kmeans', index='all')
# Load dependencies
library(robustHD)
# Build Model around standardized data
fit <- kmeans(standardize(sum_sales_percentage_2015[3:11]), 3)
# Attach clusters to dataframe
sum_sales_percentage_2015$cluster <- fit$cluster
sum_sales_percentage_2015$total_sales <- subset(sum_sales, Year==2015)$total
# Export data
write.csv(sum_sales_percentage_2015, 'data/sum_sales_percentage_2015_clusters.csv')
sum_sales_percentage_2015 %>% group_by(cluster) %>% summarise(count=n())
sum_sales_percentage_2015 %>% group_by(cluster)
sum_sales_percentage_2015 %>% group_by(cluster) %>%
summarise(dry_grocery=mean(dry_grocery),
dairy=mean(dairy),
frozen_food=mean(frozen_food),
meat=mean(meat),
produce=mean(produce),
floral=mean(floral),
deli=mean(deli),
bakery=mean(bakery),
general_merchandise=mean(general_merchandise))
demographic <- read.csv('data/storedemographicdata.csv')
info <- read.csv('data/storeinformation.csv')
sales <- read.csv('data/storesalesdata.csv')
clusters <- read.csv('data/sum_sales_percentage_2015_clusters.csv')
# Add demographic to info
demo_cluster <- demographic[0:85,2:45]
demo_cluster$cluster <- clusters$cluster
library(dplyr)
set.seed(3)
# Train (80% of observations)
demo_train <- sample_n(demo_cluster, round(0.8*nrow(info)))
# Test (20% of observations)
demo_test <- demo_cluster[!(rownames(demo_cluster) %in% rownames(demo_train)),]
# Import libraries
library(rpart)
# Grow tree
mytree <- rpart(cluster ~ ., data=demo_train)
# Display the results
printcp(mytree)
summary(mytree)
# Load dependency
library('caret')
# Test Data
predict <- data.frame('Prediction'=round(predict(mytree, demo_test)))
predict
# Confusion matrix - Yes/no are actual. False/True are predicted
cm.DecisionTree <- confusionMatrix(factor(demo_test$cluster, levels=c(1,2,3)), factor(predict$Prediction, levels=c(1,2,3)))
cm.DecisionTree
# Load dependencies
library('randomForest')
# do.trace displays the error rate vs. sample size
fit <- randomForest(cluster ~ ., data=demo_train, importance=TRUE, ntree=100, do.trace=FALSE)
# Predict test dataset
predict <- data.frame('Prediction'=round(predict(fit, demo_test, type='class')))
predict
# Confusion matrix - Yes/no are actual. False/True are predicted
cm.RandomForest <- confusionMatrix(factor(demo_test$cluster, levels=c(1,2,3)), factor(predict$Prediction, levels=c(1,2,3)))
cm.RandomForest
# Dependencies
library('gbm')
# Boosted Model
boost_model <- gbm(cluster ~ ., data=demo_train, n.trees=1580)
# Check the best number of iterations
best.iter = gbm.perf(boost_model)
best.iter
# Score the model
predict <- data.frame('Prediction'=round(predict.gbm(boost_model, newdata=demo_test, n.trees=1580, type='response')))
predict
# Make sure columns are factor type with 3 levels
demo_test$cluster <- factor(demo_test$cluster, levels=c(1,2,3))
predict$Prediction <- factor(predict$Prediction, levels=c(1,2,3))
# Confusion matrix
confusionMatrix(demo_test$cluster, predict$Prediction)
# Random Forest Model
rf_model <- randomForest(cluster ~ ., data=demo_cluster, importance=TRUE, ntree=100, do.trace=FALSE)
# Score the 10 New Stores
demo_new <- demographic[86:95,2:45]
predict <- data.frame('Prediction'=round(predict.gbm(boost_model, newdata=demo_new, n.trees=1776, type='response')))
# Add to demographic dataframe
info$cluster <- c(demo_cluster$cluster, predict$Prediction)
# Export data
write.csv(info, 'data/info_cluster.csv')
# Variable Importance Factors
varImpPlot(rf_model)
info[85:95,c(1,7)]
# Import Data
demographic <- read.csv('data/storedemographicdata.csv')
info <- read.csv('data/info_cluster.csv')
sales <- read.csv('data/storesalesdata.csv')
# Merge sales data with clusters
avg_sales_total <- merge(sales, info, by.x='Store', by.y='Store')
library(dplyr)
# Aggregate produce sales data across all stores in each cluster by month
avg_sales <- avg_sales_total %>% group_by(Store, Year, Month) %>%
summarise(avg_produce=sum(Produce)) %>%
group_by(Year, Month) %>%
summarise(avg_produce=mean(avg_produce))
# Train (to 6/15)
avg_sales_train <- head(avg_sales, 40)
# Test (7/15 to 12/15)
avg_sales_test <- tail(avg_sales, 6)
# Load dependencies
library(PerformanceAnalytics)
# Convert avg_sales to time series object
ts_train <- ts(avg_sales_train$avg_produce, start=c(2012, 3), end=c(2015, 6), frequency=12)
ts_full <- ts(avg_sales$avg_produce, start=c(2012, 3), end=c(2015, 12), frequency=12)
# Fit time series decomposition
fit <- stl(ts_train, s.window='period')
# Plot
plot(fit)
# Load dependencies
library(forecast)
# ETS Model with train dataset
fit_ets <- ets(ts_train, model='MNM')
plot(forecast(fit_ets))
# fit_ets is the model prediction
# ts_full is the actual time series object
accuracy(forecast(fit_ets, 6), ts_full[41:46])
# Plot accuracy
# Fitted in Red
plot(fit_ets$fitted, col='red')
# Actual in Blue
lines(ts_full, col='blue')
# Plot data to check if constant mean/variance
plot(ts_train)
# First Seasonal Difference
avg_sales$first_difference <- c(rep(NA,12), diff(avg_sales$avg_produce, lag=12))
# Make first_difference time series
ts_fd <- ts(avg_sales$first_difference[1:40], start=c(2012, 3), end=c(2015, 6), frequency=12)
# Plot first_difference
plot(ts_fd)
# Second, non-seasonal difference
avg_sales$second_difference <- c(NA, diff(avg_sales$first_difference, lag=1))
# Make second_difference time series
ts_sd <- ts(avg_sales$second_difference[1:40], start=c(2012, 3), end=c(2015, 6), frequency=12)
# Plot first_difference
plot(ts_sd)
# Plot the ACF of second_difference
ggAcf(avg_sales$second_difference[1:40], lag.max=48)
# Plot the PACF of second_difference
ggPacf(avg_sales$second_difference[1:40], lag.max=48)
# Build Auto Arima Model
fit_arima <- Arima(ts_train, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
fit_arima
# fit_arima is the model prediction
# ts_full is the actual time series object
accuracy(forecast(fit_arima, 6), ts_full[41:46])
# Plot accuracy
# Fitted in Blue
plot(forecast(fit_arima))
# Actual in Red
lines(ts_full, col='red')
# Arima Model with all data
fit_bm <- fit_ets <- ets(ts_full, model='MNM')
plot(forecast(fit_bm))
# ARIMA Model with all data
forecast <- forecast(fit_bm, 12)
forecast
# Build forecast tables
forecasts <- data.frame('Existing Stores' = forecast$mean*85,
'New Stores' = forecast$mean*10)
# Change rownames
rownames(forecasts) <- c('Jan-16', 'Feb-16', 'Mar-16', 'Apr-16', 'May-16', 'Jun-16', 'Jul-16', 'Aug-16', 'Sep-16', 'Oct-16', 'Nov-16', 'Dec-16')
# Display table
forecasts
# Export data for data visualization
# Total monthly sales
# Forecast
sales_forecast <- data.frame('Year'=as.integer(rep(c(2016),12)),
'Month'=as.integer(seq(1,12,1)),
'Existing'=as.numeric(forecasts$Existing.Stores),
'New'=as.numeric(forecasts$New.Stores))
# Past sales
sales_past <- avg_sales_total %>% group_by(Year, Month) %>%
summarise(avg_produce=sum(Produce))
# Timeframe
dates <- seq(as.Date("2012/3/1"), as.Date("2016/12/1"), by = "month")
# Existing
existing_data <- c(sales_past$avg_produce, sales_forecast$Existing)
# New
new_data <- c(rep(NA,46),sales_forecast$New)
# Combine for all sales data
sales_forecasts <- data.frame('Date'=dates,
'Existing'=existing_data,
'New'=new_data)
# Export data
write.csv(sales_forecasts, 'data/sales_forecasts.csv')
sum_sales_percentage_2015 %>% group_by(cluster) %>% summarise(count=n())
sum_sales_percentage_2015 %>% group_by(cluster)
sum_sales_percentage_2015 %>% group_by(cluster) %>%
summarise(dry_grocery=mean(dry_grocery),
dairy=mean(dairy),
frozen_food=mean(frozen_food),
meat=mean(meat),
produce=mean(produce),
floral=mean(floral),
deli=mean(deli),
bakery=mean(bakery),
general_merchandise=mean(general_merchandise))
# Variable Importance Factors
varImpPlot(rf_model)
info[85:95,c(1,7)]
# Build forecast tables
forecasts <- data.frame('Existing Stores' = forecast$mean*85,
'New Stores' = forecast$mean*10)
# Change rownames
rownames(forecasts) <- c('Jan-16', 'Feb-16', 'Mar-16', 'Apr-16', 'May-16', 'Jun-16', 'Jul-16', 'Aug-16', 'Sep-16', 'Oct-16', 'Nov-16', 'Dec-16')
# Display table
forecasts
knit_with_parameters('~/Google Drive/Computer Science/Udacity/Business Analyst/P7 - Segmentation & Clustering/Project/Project 7.Rmd')
demographic <- read.csv('data/storedemographicdata.csv')
info <- read.csv('data/storeinformation.csv')
sales <- read.csv('data/storesalesdata.csv')
library(dplyr)
# Sum sales categories by store and year
sum_sales <- sales %>% group_by(Store, Year) %>%
summarise(dry_grocery=sum(Dry_Grocery),
dairy=sum(Dairy),
frozen_food=sum(Frozen_Food),
meat=sum(Meat),
produce=sum(Produce),
floral=sum(Floral),
deli=sum(Deli),
bakery=sum(Bakery),
general_merchandise=sum(General_Merchandise))
# Sum of all categories
sum_sales$total <- sum_sales$dry_grocery + sum_sales$dairy + sum_sales$frozen_food + sum_sales$meat + sum_sales$produce + sum_sales$floral + sum_sales$deli + sum_sales$bakery + sum_sales$general_merchandise
# Determine percentage of total
sum_sales_percentage <- data.frame(store=sum_sales$Store,
year=sum_sales$Year,
dry_grocery=100*sum_sales$dry_grocery/sum_sales$total,
dairy=100*sum_sales$dairy/sum_sales$total,
frozen_food=100*sum_sales$frozen_food/sum_sales$total,
meat=100*sum_sales$meat/sum_sales$total,
produce=100*sum_sales$produce/sum_sales$total,
floral=100*sum_sales$floral/sum_sales$total,
deli=100*sum_sales$deli/sum_sales$total,
bakery=100*sum_sales$bakery/sum_sales$total,
general_merchandise=100*sum_sales$general_merchandise/sum_sales$total)
# Check to make sure each column adds up to 100
sum_sales_percentage$total <- sum_sales_percentage$dry_grocery + sum_sales_percentage$dairy + sum_sales_percentage$frozen_food + sum_sales_percentage$meat + sum_sales_percentage$produce + sum_sales_percentage$floral + sum_sales_percentage$deli + sum_sales_percentage$bakery + sum_sales_percentage$general_merchandise
# Obtain 2015 sales data
sum_sales_percentage_2015 <- subset(sum_sales_percentage, year==2015)
# Load dependencies
library(NbClust)
# Determine optimal number of clusters
NbClust(sum_sales_percentage_2015[3:11], method='kmeans', index='ch')
summary <- NbClust(sum_sales_percentage_2015[3:11], method='kmeans', index='all')
# Load dependencies
library(robustHD)
# Build Model around standardized data
fit <- kmeans(standardize(sum_sales_percentage_2015[3:11]), 3)
# Attach clusters to dataframe
sum_sales_percentage_2015$cluster <- fit$cluster
sum_sales_percentage_2015$total_sales <- subset(sum_sales, Year==2015)$total
# Export data
write.csv(sum_sales_percentage_2015, 'data/sum_sales_percentage_2015_clusters.csv')
sum_sales_percentage_2015 %>% group_by(cluster) %>% summarise(count=n())
sum_sales_percentage_2015 %>% group_by(cluster)
sum_sales_percentage_2015 %>% group_by(cluster) %>%
summarise(dry_grocery=mean(dry_grocery),
dairy=mean(dairy),
frozen_food=mean(frozen_food),
meat=mean(meat),
produce=mean(produce),
floral=mean(floral),
deli=mean(deli),
bakery=mean(bakery),
general_merchandise=mean(general_merchandise))
demographic <- read.csv('data/storedemographicdata.csv')
info <- read.csv('data/storeinformation.csv')
sales <- read.csv('data/storesalesdata.csv')
clusters <- read.csv('data/sum_sales_percentage_2015_clusters.csv')
# Add demographic to info
demo_cluster <- demographic[0:85,2:45]
demo_cluster$cluster <- clusters$cluster
library(dplyr)
set.seed(3)
# Train (80% of observations)
demo_train <- sample_n(demo_cluster, round(0.8*nrow(info)))
# Test (20% of observations)
demo_test <- demo_cluster[!(rownames(demo_cluster) %in% rownames(demo_train)),]
# Import libraries
library(rpart)
# Grow tree
mytree <- rpart(cluster ~ ., data=demo_train)
# Display the results
printcp(mytree)
summary(mytree)
# Load dependency
library('caret')
# Test Data
predict <- data.frame('Prediction'=round(predict(mytree, demo_test)))
predict
# Confusion matrix - Yes/no are actual. False/True are predicted
cm.DecisionTree <- confusionMatrix(factor(demo_test$cluster, levels=c(1,2,3)), factor(predict$Prediction, levels=c(1,2,3)))
cm.DecisionTree
# Load dependencies
library('randomForest')
# do.trace displays the error rate vs. sample size
fit <- randomForest(cluster ~ ., data=demo_train, importance=TRUE, ntree=100, do.trace=FALSE)
# Predict test dataset
predict <- data.frame('Prediction'=round(predict(fit, demo_test, type='class')))
predict
# Confusion matrix - Yes/no are actual. False/True are predicted
cm.RandomForest <- confusionMatrix(factor(demo_test$cluster, levels=c(1,2,3)), factor(predict$Prediction, levels=c(1,2,3)))
cm.RandomForest
# Dependencies
library('gbm')
# Boosted Model
boost_model <- gbm(cluster ~ ., data=demo_train, n.trees=1580)
# Check the best number of iterations
best.iter = gbm.perf(boost_model)
best.iter
# Score the model
predict <- data.frame('Prediction'=round(predict.gbm(boost_model, newdata=demo_test, n.trees=1580, type='response')))
predict
# Make sure columns are factor type with 3 levels
demo_test$cluster <- factor(demo_test$cluster, levels=c(1,2,3))
predict$Prediction <- factor(predict$Prediction, levels=c(1,2,3))
# Confusion matrix
confusionMatrix(demo_test$cluster, predict$Prediction)
# Random Forest Model
rf_model <- randomForest(cluster ~ ., data=demo_cluster, importance=TRUE, ntree=100, do.trace=FALSE)
# Score the 10 New Stores
demo_new <- demographic[86:95,2:45]
predict <- data.frame('Prediction'=round(predict.gbm(boost_model, newdata=demo_new, n.trees=1776, type='response')))
# Add to demographic dataframe
info$cluster <- c(demo_cluster$cluster, predict$Prediction)
# Export data
write.csv(info, 'data/info_cluster.csv')
# Variable Importance Factors
varImpPlot(rf_model)
info[85:95,c(1,7)]
# Import Data
demographic <- read.csv('data/storedemographicdata.csv')
info <- read.csv('data/info_cluster.csv')
sales <- read.csv('data/storesalesdata.csv')
# Merge sales data with clusters
avg_sales_total <- merge(sales, info, by.x='Store', by.y='Store')
library(dplyr)
# Aggregate produce sales data across all stores in each cluster by month
avg_sales <- avg_sales_total %>% group_by(Store, Year, Month) %>%
summarise(avg_produce=sum(Produce)) %>%
group_by(Year, Month) %>%
summarise(avg_produce=mean(avg_produce))
# Train (to 6/15)
avg_sales_train <- head(avg_sales, 40)
# Test (7/15 to 12/15)
avg_sales_test <- tail(avg_sales, 6)
# Load dependencies
library(PerformanceAnalytics)
# Convert avg_sales to time series object
ts_train <- ts(avg_sales_train$avg_produce, start=c(2012, 3), end=c(2015, 6), frequency=12)
ts_full <- ts(avg_sales$avg_produce, start=c(2012, 3), end=c(2015, 12), frequency=12)
# Fit time series decomposition
fit <- stl(ts_train, s.window='period')
# Plot
plot(fit)
# Load dependencies
library(forecast)
# ETS Model with train dataset
fit_ets <- ets(ts_train, model='MNM')
plot(forecast(fit_ets))
# fit_ets is the model prediction
# ts_full is the actual time series object
accuracy(forecast(fit_ets, 6), ts_full[41:46])
# Plot accuracy
# Fitted in Red
plot(fit_ets$fitted, col='red')
# Actual in Blue
lines(ts_full, col='blue')
# Plot data to check if constant mean/variance
plot(ts_train)
# First Seasonal Difference
avg_sales$first_difference <- c(rep(NA,12), diff(avg_sales$avg_produce, lag=12))
# Make first_difference time series
ts_fd <- ts(avg_sales$first_difference[1:40], start=c(2012, 3), end=c(2015, 6), frequency=12)
# Plot first_difference
plot(ts_fd)
# Second, non-seasonal difference
avg_sales$second_difference <- c(NA, diff(avg_sales$first_difference, lag=1))
# Make second_difference time series
ts_sd <- ts(avg_sales$second_difference[1:40], start=c(2012, 3), end=c(2015, 6), frequency=12)
# Plot first_difference
plot(ts_sd)
# Plot the ACF of second_difference
ggAcf(avg_sales$second_difference[1:40], lag.max=48)
# Plot the PACF of second_difference
ggPacf(avg_sales$second_difference[1:40], lag.max=48)
# Build Auto Arima Model
fit_arima <- Arima(ts_train, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
fit_arima
# fit_arima is the model prediction
# ts_full is the actual time series object
accuracy(forecast(fit_arima, 6), ts_full[41:46])
# Plot accuracy
# Fitted in Blue
plot(forecast(fit_arima))
# Actual in Red
lines(ts_full, col='red')
# Arima Model with all data
fit_bm <- fit_ets <- ets(ts_full, model='MNM')
plot(forecast(fit_bm))
# ARIMA Model with all data
forecast <- forecast(fit_bm, 12)
forecast
# Build forecast tables
forecasts <- data.frame('Existing Stores' = forecast$mean*85,
'New Stores' = forecast$mean*10)
# Change rownames
rownames(forecasts) <- c('Jan-16', 'Feb-16', 'Mar-16', 'Apr-16', 'May-16', 'Jun-16', 'Jul-16', 'Aug-16', 'Sep-16', 'Oct-16', 'Nov-16', 'Dec-16')
# Display table
forecasts
# Export data for data visualization
# Total monthly sales
# Forecast
sales_forecast <- data.frame('Year'=as.integer(rep(c(2016),12)),
'Month'=as.integer(seq(1,12,1)),
'Existing'=as.numeric(forecasts$Existing.Stores),
'New'=as.numeric(forecasts$New.Stores))
# Past sales
sales_past <- avg_sales_total %>% group_by(Year, Month) %>%
summarise(avg_produce=sum(Produce))
# Timeframe
dates <- seq(as.Date("2012/3/1"), as.Date("2016/12/1"), by = "month")
# Existing
existing_data <- c(sales_past$avg_produce, sales_forecast$Existing)
# New
new_data <- c(rep(NA,46),sales_forecast$New)
# Combine for all sales data
sales_forecasts <- data.frame('Date'=dates,
'Existing'=existing_data,
'New'=new_data)
# Export data
write.csv(sales_forecasts, 'data/sales_forecasts.csv')
sum_sales_percentage_2015 %>% group_by(cluster) %>% summarise(count=n())
sum_sales_percentage_2015 %>% group_by(cluster)
sum_sales_percentage_2015 %>% group_by(cluster) %>%
summarise(dry_grocery=mean(dry_grocery),
dairy=mean(dairy),
frozen_food=mean(frozen_food),
meat=mean(meat),
produce=mean(produce),
floral=mean(floral),
deli=mean(deli),
bakery=mean(bakery),
general_merchandise=mean(general_merchandise))
# Variable Importance Factors
varImpPlot(rf_model)
info[85:95,c(1,7)]
# Build forecast tables
forecasts <- data.frame('Existing Stores' = forecast$mean*85,
'New Stores' = forecast$mean*10)
# Change rownames
rownames(forecasts) <- c('Jan-16', 'Feb-16', 'Mar-16', 'Apr-16', 'May-16', 'Jun-16', 'Jul-16', 'Aug-16', 'Sep-16', 'Oct-16', 'Nov-16', 'Dec-16')
# Display table
forecasts
sum_sales_percentage_2015 %>% group_by(cluster) %>% summarise(count=n())
sum_sales_percentage_2015 %>% group_by(cluster)
sum_sales_percentage_2015 %>% group_by(cluster) %>%
summarise(dry_grocery=mean(dry_grocery),
dairy=mean(dairy),
frozen_food=mean(frozen_food),
meat=mean(meat),
produce=mean(produce),
floral=mean(floral),
deli=mean(deli),
bakery=mean(bakery),
general_merchandise=mean(general_merchandise))
# Variable Importance Factors
varImpPlot(rf_model)
info[85:95,c(1,7)]
# Build forecast tables
forecasts <- data.frame('Existing Stores' = forecast$mean*85,
'New Stores' = forecast$mean*10)
# Change rownames
rownames(forecasts) <- c('Jan-16', 'Feb-16', 'Mar-16', 'Apr-16', 'May-16', 'Jun-16', 'Jul-16', 'Aug-16', 'Sep-16', 'Oct-16', 'Nov-16', 'Dec-16')
# Display table
forecasts
knit_with_parameters('~/Google Drive/Computer Science/Udacity/Business Analyst/P7 - Segmentation & Clustering/Project/P7_Conclusions.Rmd')
rmarkdown::render("your_doc.Rmd")
rmarkdown::render("P7_Conclusions.Rmd")
getwd()
setwd('~/Google Drive/Computer Science/Udacity/Business Analyst/P7 - Segmentation & Clustering')
getwd()
list.files()
setwd('~/Google Drive/Computer Science/Udacity/Business Analyst/P7 - Segmentation & Clustering/Project')
list.files()
rmarkdown::render("P7_Conclusions.Rmd")
